---
title: Math 10
footer: true
math: true
---

## S22 version

## Class 1

## Class 2

## Class 3

## Class 4

## Class 5

## Class 6

## Class 7

## Class 8

## Class 9

## Class 10

## Class 11

## Class 12

## Class 13

## Class 14

## Class 15

## Class 16

## Class 17

## Class 18

## Class 19

Here is some math $3x^2 + 10$ how do you think that looks?

## Class 20

## Class 21

## Class 22

## Class 23

## Class 24

## Class 25

## Class 26

## Class 27

## Class 28

## Class 29

Introduction.  Where to get help.  Programming environment.  Variables and data types in Python (including NumPy). for loops, if statements, and while loops.  Comparison of speed.  Indexing and slicing with NumPy.    Pythonic code and DRY code.

Data types
* list
* string
* dict
* tuple
* set
* int
* float
* bool
* function

Concepts used by experienced Python programmers:
* list comprehension
* f-strings
* lambda functions

## Week 2

Introduction to pandas.   Examples of the following in the context of pandas (for example, with regards to `apply` and `map` and `applymap`):
* list comprehension
* f-strings
* lambda functions

Using `where` from NumPy and pandas styler.

Cleaning a dataset, for example dropping (and specifying) null values.

Practice finding a dataset from Kaggle.

## Week 3

Plotting based on the *Grammar of Graphics* using Altair, and a quick comparison with Seaborn and Plotly.  Possible options: Altair, Seaborn, Plotly, pandas plotting.  Relation of data type to how Altair displays the data.  Adding a title, tooltip, using f-strings.  Displaying multiple charts using tuple unpacking and `alt.vconcat` or `alt.hconcat`.

## Week 4

Basic concepts of Data Science.  (Feature engineering.)
Review from Math 9.  Programming environment for the class (Jupyter notebooks or Google Colab or Deepnote).  Basic concepts of Data Science.  Supervised vs unsupervised learning.  Regression vs classification.

Feature engineering.  K-nearest neighbors classification and K-nearest neighbors regression.  Importance of scaling the data.  Evaluating the performance using relevant loss functions.  In the classification case, why do we use `log_loss` or another loss function but not `mean_squared_error` or `mean_absolute_error`?  The importance of having a test set, and making one using `train_test_split`.The notion of a decision boundary.    Over-fitting and under-fitting and the bias-variance tradeoff.

## Week 5

Midterm.  Start on Week 6 material.

## Week 6

Scikit-learn.  Linear regression.  Cost function/loss function.  Finding coefficients via the normal equation and via gradient descent.  Polynomial regression.  More on over-fitting and under-fitting.    Interpreting the coefficients.  Regularization.

## Week 7

Logistic regression.  Why is logistic regression also considered a *linear* model?  How are the coefficients found in this case?

## Week 8

Support vector machines.

## Week 9

Tree-based models and random forests. 

## Week 10

Preview of advanced methods.  Boosting.

## Finals Week

Final exam and/or Course project.

## Future version:

## Class 1

## Class 2

## Class 3

## Class 4

## Class 5

## Class 6

## Class 7

## Class 8

## Class 9

## Class 10

## Class 11

## Class 12

## Class 13

## Class 14

## Class 15

## Class 16

## Class 17

## Class 18

## Class 19

## Class 20

## Class 21

## Class 22

## Class 23

## Class 24

## Class 25

## Class 26

## Class 27

## Class 28

## Class 29

## Week 1

Introduction.  Review from Math 9.  Programming environment for the class (Jupyter notebooks or Google Colab or Deepnote).  Basic concepts of Data Science.  Supervised vs unsupervised learning.  Regression vs classification.

## Week 2

Introduction to pandas.  Review of the following in the context of pandas (for example, with regards to `apply` and `map` and `applymap`):
* list comprehension
* f-strings
* lambda functions

Cleaning a dataset, for example by dropping (and specifying) null values.

Using `where` from NumPy and pandas styler.

## Week 3

Plotting based on the *Grammar of Graphics*.  Possible options: Altair, Seaborn, Plotly, pandas plotting.  Relation of data type to how Altair displays the data.

## Week 4

Feature engineering.  K-nearest neighbors classification and K-nearest neighbors regression.  Importance of scaling the data.  Evaluating the performance using relevant loss functions.  In the classification case, why do we use `log_loss` or another loss function but not `mean_squared_error` or `mean_absolute_error`?  The importance of having a test set, and making one using `train_test_split`.The notion of a decision boundary.    Over-fitting and under-fitting and the bias-variance tradeoff.

## Week 5

Midterm.  Start on Week 6 material.

## Week 6

Scikit-learn.  Linear regression.  Cost function/loss function.  Finding coefficients via the normal equation and via gradient descent.  Polynomial regression.  More on over-fitting and under-fitting.    Interpreting the coefficients.  Regularization.

## Week 7

Logistic regression.  Why is logistic regression also considered a *linear* model?  How are the coefficients found in this case?

## Week 8

Support vector machines.

## Week 9

Tree-based models and random forests. 

## Week 10

Preview of advanced methods.  Boosting.

## Finals Week

Final exam and/or Course project.